{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52f6b107-1532-4fdf-ad0a-833205c09902",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For more detail:\n",
    "https://tree.rocks/make-language-model-from-scratch-like-mnist-5ed59aeb538d\n",
    "\"\"\"\n",
    "# !pip install torch numpy einops tqdm matplotlib scikit-learn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import einops\n",
    "import string\n",
    "import re\n",
    "from tqdm.auto import trange\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5452f67e-9862-431c-9fc2-73a768b6271d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print('device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d7dea7-8e26-4bec-bec3-6e5aed26f8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = {}\n",
    "id2word = {}\n",
    "\n",
    "def format_number(num):\n",
    "    return f\"{num:,}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b46996e-7edd-4bf8-9ab6-2585df804bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/short_animal_texts.txt', 'r') as f:\n",
    "        text_data = f.read()\n",
    "\n",
    "total_characters = len(text_data)\n",
    "print('total_characters:', format_number(total_characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90bcbdb-afc6-4d35-a9e4-b0f97f142a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_tokenizer(text):\n",
    "    return re.findall(r'\\w+|[^\\w\\s]|[\\s]+', text, re.UNICODE)\n",
    "\n",
    "print(regex_tokenizer(\"Hi, It's sunny day!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ac7ea9-7be1-45ab-82da-96af949f97b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_words = regex_tokenizer(text_data)\n",
    "unique_words = set(cleaned_words)\n",
    "print('unique_words:', len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fe70de-dc05-4281-8469-c0a02adb250a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_unique_words = sorted(unique_words)\n",
    "for i, w in enumerate(sorted_unique_words):\n",
    "    word2id[w] = i\n",
    "    id2word[i] = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0134dd-c3e9-415e-b2bb-8510e801bb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text):\n",
    "    tokens = regex_tokenizer(text)\n",
    "    return [word2id[w] for w in tokens]\n",
    "\n",
    "def decode(token_ids):\n",
    "    return ''.join([id2word[i] for i in token_ids])\n",
    "\n",
    "print(encode(\"Hi, It's sunny day!\"))\n",
    "print(decode(encode(\"Hi, It's sunny day!\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ca8551-2a0a-4aa7-91e1-26d5dd6b4c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    \"num_unique_words\": len(unique_words),\n",
    "    \"context_length\": 384,\n",
    "\n",
    "    \"emb_dim\": 128,\n",
    "    \"head_dim\": 384,\n",
    "\n",
    "    \"drop_rate\": 0.15,\n",
    "\n",
    "    \"stride\": 8,\n",
    "    \"batch_size\": 32,\n",
    "    \"LR\": 0.0009,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae55384e-6cd4-4cc2-96b2-56015a398b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, txt, cfg):\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "\n",
    "        token_ids = encode(txt)\n",
    "        c = cfg['context_length']\n",
    "        for i in range(0, len(token_ids) - c + 1, c // cfg['stride']):\n",
    "            self.x.append(torch.tensor(token_ids[i:i + c]))\n",
    "            self.y.append(torch.tensor(token_ids[i + 1:i + c + 1]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "def create_dataloader(text):\n",
    "    ds = TextDataset(text, CFG)\n",
    "    loader = DataLoader(\n",
    "        ds,\n",
    "        batch_size=CFG['batch_size'],\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    return loader\n",
    "\n",
    "train_loader = create_dataloader(text_data)\n",
    "\n",
    "x, y = next(iter(train_loader))\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecc64e0-4d3b-41df-a912-8c097a817ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "print(decode(x[0].tolist()[:20]))\n",
    "print(decode(y[0].tolist()[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ac2c05-b177-4e83-b181-129e33b7f0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.embedding = nn.Embedding(cfg['num_unique_words'], cfg['emb_dim'])\n",
    "        self.pos_emb = nn.Embedding(cfg['context_length'], cfg['emb_dim'])\n",
    "\n",
    "        self.w_q = nn.Linear(cfg['emb_dim'], cfg['head_dim'], bias=False)\n",
    "        self.w_k = nn.Linear(cfg['emb_dim'], cfg['head_dim'], bias=False)\n",
    "        self.w_v = nn.Linear(cfg['emb_dim'], cfg['head_dim'], bias=False)\n",
    "\n",
    "        self.dropout_input = nn.Dropout(cfg['drop_rate'])\n",
    "        self.dropout_attention = nn.Dropout(cfg['drop_rate'])\n",
    "\n",
    "        self.norm = nn.LayerNorm(cfg['head_dim'])\n",
    "        self.output = nn.Linear(cfg['head_dim'], cfg['emb_dim'], bias=False)\n",
    "\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(cfg['context_length'], cfg['context_length']), diagonal=1 ).bool())\n",
    "\n",
    "    def forward(self, x_input):\n",
    "        b, n = x_input.shape\n",
    "        x_emb = self.embedding(x_input)\n",
    "        x_pos = self.pos_emb(torch.arange(n, device=x_input.device))\n",
    "        \n",
    "\n",
    "        x = self.dropout_input(x_emb + x_pos)\n",
    "        head_dim = self.cfg['head_dim']\n",
    "\n",
    "        \n",
    "        w_q = self.w_q(x)\n",
    "        w_k = self.w_k(x)\n",
    "        w_v = self.w_v(x)\n",
    "\n",
    "        attention_score = (w_q @ w_k.transpose(-1, -2)) / (head_dim ** 0.5)\n",
    "\n",
    "        mask = self.mask[:n,:n]\n",
    "        attention_score = attention_score.masked_fill(mask, -torch.inf)\n",
    "\n",
    "        attention_weight = torch.softmax(attention_score, dim=-1)\n",
    "        attention_weight = self.dropout_attention(attention_weight)\n",
    "        \n",
    "        x = attention_weight @ w_v\n",
    "        x = self.norm(x)\n",
    "        x = nn.functional.gelu(x)\n",
    "\n",
    "        x = self.output(x)\n",
    "        x = x @ self.embedding.weight.T\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8dbf36-eba2-407e-9d66-4a1507d1d14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(CFG)\n",
    "print(model(torch.randint(0, len(unique_words), size=(5, 8))).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dfec83-72f4-4837-8d62-aaef4ddb33c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_trainable_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print('Model paramters:', format_number(count_trainable_parameters(model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6ef7c6-0d77-4eda-b341-441c15dbe80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CFG['LR'], weight_decay=0.1)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cd3767-f1a3-44f3-8351-f0f6257e948b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_embedding(sample_count=7):\n",
    "    tags = [\n",
    "        'cat',\n",
    "        'tree',\n",
    "        'blue',\n",
    "        'Bob',\n",
    "        'jump',\n",
    "        'friendly',\n",
    "    ]\n",
    "\n",
    "    tags_i = [word2id[t] for t in tags]\n",
    "\n",
    "    weights = model.embedding.weight.detach().cpu().numpy()\n",
    "\n",
    "    def query(idx):\n",
    "        sel = weights[idx].reshape(1, weights.shape[1])\n",
    "        score = (sel @ weights.T).squeeze()\n",
    "\n",
    "        score = [(s, id2word[i], i) for i, s in enumerate(score)]\n",
    "        score = sorted(score, reverse=True)\n",
    "        score = score[:sample_count]\n",
    "\n",
    "        result = [f'{n}: {s:.3f}' for s, n, _ in score]\n",
    "        print(id2word[idx], '->')\n",
    "        print(', '.join(result))\n",
    "        print('\\n')\n",
    "        return [i for _, _, i in score]\n",
    "\n",
    "\n",
    "    arr = []\n",
    "    for i in tags_i:\n",
    "        arr += query(i)\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced = pca.fit_transform(weights[arr])\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(reduced[:, 0], reduced[:, 1], s=20, alpha=0.7)\n",
    "    for i in range(len(reduced)):\n",
    "        label = id2word[arr[i]]\n",
    "        attr = {\n",
    "            'fontsize': 8,\n",
    "        }\n",
    "        if arr[i] in tags_i:\n",
    "            attr['fontsize'] = 10\n",
    "            attr['fontweight'] = 'bold'\n",
    "        else:\n",
    "            attr['alpha'] = 0.6\n",
    "        plt.text(reduced[i, 0], reduced[i, 1], label, **attr)\n",
    "    \n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93988a2-2e2c-4a65-a029-b727150ca337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, max_len=50):\n",
    "    model.eval()\n",
    "\n",
    "    token_ids = encode(text)\n",
    "    token_ids = torch.tensor(token_ids).to(device)\n",
    "    token_ids = token_ids.unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            token_ids = token_ids[:, -CFG['context_length']:]\n",
    "            y = model(token_ids)\n",
    "            y = y[:, -1, :]\n",
    "            y_probs = torch.softmax(y, dim=-1)\n",
    "            y_next = torch.argmax(y_probs, dim=-1, keepdim=True)\n",
    "            token_ids = torch.cat([token_ids, y_next], dim=-1)\n",
    "\n",
    "    token_ids = token_ids.squeeze().tolist()\n",
    "    output_text = decode(token_ids)\n",
    "    print(output_text)\n",
    "    model.train()\n",
    "\n",
    "predict('In a sunny day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ca8d07-2625-4130-adb8-6557bd523f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(x, y_true):\n",
    "    y = model(x)\n",
    "    return torch.nn.functional.cross_entropy(y.flatten(0, 1), y_true.flatten())\n",
    "    \n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    t = min(len(loader), 30)\n",
    "    total_loss, total_count = 0.0, 0\n",
    "    iloader = iter(loader)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(t):\n",
    "            x, y_true = next(iloader)\n",
    "            loss = calc_loss(x.to(device), y_true.to(device))\n",
    "            total_loss += loss\n",
    "\n",
    "    model.train()\n",
    "    return total_loss / t\n",
    "\n",
    "evaluate(train_loader).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67db927e-32b3-4ae8-888f-4e36f36c30c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b301a5e1-9d3d-4bee-8320-5cc4391b5193",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_text = 'In a sunny day'\n",
    "\n",
    "def train(epochs=50):\n",
    "    bar = trange(epochs)\n",
    "    tlen = len(train_loader)\n",
    "    for i in bar:\n",
    "        model.train()\n",
    "        for j, (x, y_true) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss(x.to(device), y_true.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            bar.set_description(f'Epochs: {i+1}/{epochs}, Batch: {j+1}/{tlen}, loss: {loss.item():.5f}')\n",
    "\n",
    "        val_loss = evaluate(train_loader).item()\n",
    "        print(f'val loss: {val_loss:.5f}')\n",
    "\n",
    "        if i % 5 == 0:\n",
    "            print(f'predict {i+1} >>')\n",
    "            predict(pred_text, max_len=50)\n",
    "            print('\\n')\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f71596f-ef79-4fea-9fad-a47a11175ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daff768c-4086-463e-9ee5-dc1290d60c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict('Once upon a time', max_len=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8d2a83-e02c-4394-a909-a8adafc07b0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
